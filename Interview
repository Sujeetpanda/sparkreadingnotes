1) How take works on partitioned rdd. From with partition it will pick : partition 0
2) How to do a effectively 100 gb and 80gb files join :
First, re-partition the data and persist using partitioned tables (dataframe.write.partitionBy()). 
Then, join sub-partitions serially in a loop, "appending" to the same final result table.(SaveMode.Overwrite)
3) https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa
4) If i am executing 50 gb file out of that 5 gb processed.But the job stoppped. How to activate the job in best possible way.
5) How DS does compile time check. : Lambda and JVM objects
6) Memory Usage of Reduce Tasks : Increase the label of parallalism : park.default.parallelism
09.10.2021
=======================
7) The recommended number of partitions is between two to three times the number of executors. 
In our case, 600 = 10 x 24 x 2.5 would be an appropriate number of partitions. 10 nodes with 24 cpu cores
8) Types of deployment mode in spark 
  ./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \       spark-submit --deploy-mode client --driver-memory xxxx  ......
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
  
  9) Executor tuning:
  
    -- Num executors
    -- executor cores
    -- executor memory
